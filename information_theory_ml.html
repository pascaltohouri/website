<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="An essay by Pascal Tohouri on machine learning experiment traversals, and formal languages.">
    <title>Linguistic Descriptions of Experiment Traversals - Pascal Tohouri</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- KaTeX CSS and JS for Math Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lora:ital,wght@0,400;0,600;1,400&family=Roboto:wght@400;500&family=Roboto+Slab:wght@400;700&display=swap" rel="stylesheet">

    <style>
        /* Typography Settings */
        body {
            font-family: 'Lora', Georgia, sans-serif;
            -webkit-font-smoothing: antialiased;
        }

        /* Headers and UI Fonts */
        h1, h2, h3, h4 {
            font-family: 'Roboto Slab', sans-serif;
        }
        
        nav, footer, .meta-data {
            font-family: 'Roboto', sans-sans-serif;
        }

        /* LaTeX-style Numbering System */
        .article-body {
            counter-reset: section;
        }
        .article-body p {
            margin-bottom: 1.25rem;
            line-height: 1.8; /* Improved line height for reading */
            font-size: 1.125rem; /* 18px base size */
            color: #374151;
            text-align: justify;         /* ðŸ‘ˆ add this */
            text-justify: inter-word;    /* optional hint for some browsers */
        }

        .article-body h2 {
            font-size: 1.75rem;
            font-weight: 700;
            margin-top: 2.5rem;
            margin-bottom: 1rem;
            line-height: 1.3;
            color: #111827; /* Gray 900 */
            counter-reset: subsection;
        }

        .article-body h2::before {
            counter-increment: section;
            content: counter(section) ". ";
        }

        .article-body h3 {
            font-size: 1.35rem;
            font-weight: 700;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            line-height: 1.3;
            color: #374151; /* Gray 700 */
            counter-reset: subsubsection;
        }

        .article-body h3::before {
            counter-increment: subsection;
            content: counter(section) "." counter(subsection) " ";
        }

        .article-body p {
            margin-bottom: 1.25rem;
            line-height: 1.8; /* Improved line height for reading */
            font-size: 1.125rem; /* 18px base size */
            color: #374151;
        }

        /* Link Styling */
        .article-link {
            color: #2563eb;
            text-decoration: underline;
            text-decoration-thickness: 1px;
            text-underline-offset: 2px;
        }
        .article-link:hover {
            color: #1e40af;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <header class="bg-white shadow-sm sticky top-0 z-50 border-b border-gray-100">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center max-w-4xl">
            <a href="#" class="text-xl font-bold text-gray-900 tracking-tight">Pascal Tohouri</a>
            <a href="#" class="text-sm font-medium text-gray-500 hover:text-gray-900 transition duration-300 group">
                <span class="inline-block transition-transform group-hover:-translate-x-1">&larr;</span> Back to Home
            </a>
        </nav>
    </header>

    <main>
        <article class="container mx-auto px-4 py-12 md:py-16 max-w-3xl"> <!-- Narrower max-w-3xl for readability -->
            <div class="bg-white shadow-sm rounded-xl px-6 md:px-12 py-10 border border-gray-100">
                
                <header class="mb-10 text-center">
                    <h1 class="text-3xl md:text-5xl font-bold text-gray-900 leading-tight mb-4">Linguistic Descriptions of Experiment Traversals</h1>
                    <div class="meta-data flex justify-center gap-4 text-sm text-gray-500 uppercase tracking-wider">
                        <span>Nov 18, 2025</span>
                        <span>&bull;</span>
                        <span>6 Min Read</span>
                    </div>
                </header>

                <div class="article-body">
                    <h2>Learning Representations and Symbolic Information</h2>
                    <h3>Essential Experiment Features</h3>
                    <p>A machine learning experiment comprises many different elements. For example, when training an LLM, one specifies the architecture (attention heads, hidden dimension size, normalisation strategy...), loss function, hyper-parameters, learning rate \(\eta\), etc.. These elements make the experiment unique; changing them would recover a different experiment \(e'\).</p>
                    
                    <h3>Differing Representations</h3>
                    <p>The symbolic information that each experiment contains is like an abstract symmetry preserved across different representations. For example, one might represent an experiment (where an LLM is trained via SGD) as a programming script, a semantic vector \(\vec{v}_e\), or in natural language. Each representation has different linguistic rules, and thus different properties (e.g., completeness, description lengths, self-referencing), but the essence of an experiment is preserved when translating between representations.</p>
                    
                    <h3>Platonic and Structuralist Forms</h3>
                    <p>In the Platonic view, each experiment's abstract form resides in the canonical space of forms; a symbolic space of semantically distinct objects. In the structuralist view, no independent space of forms exists; an experiment is just the enduring symmetries that translate between representations. One unifying interpretation is the manifold hypothesis, which unites these views by saying each semantic object in the symbolic space sits on a manifold and has an associated fiber containing all possible representations. So, the symbolic representation of "LLM" has a mapping to each representation including "programming script", "technical report", "semantic vector", and so forth.</p>
                    <p>A manifold of fiber bundles is a Moduli space<sup class="footnote-ref"><a href="#fn1" id="fnref1">1</a></sup>, and this interpretation is problematic because it presupposes some structure onto the canonical space of forms. For example, it is intuitively disquieting that a fundamental, abstract space should happen to be locally Euclidean. Arguably therefore, this canonical representation is just as valid as any other representation, and the structuralist view of persistent symmetric properties is more reasonable. Still, the manifold hypothesis is useful here for illustrative purposes.

                    <h3>Method-Specific Limitations</h3>
                    <p>A task with well-defined preferences over all these symbolic experiments may have an optimum \(e^*\), but holding some experiment features fixed, the researcher may never find it. For example, sticking to an LLM architecture may guarantee another superior architecture is never discovered, irrespective of the gains from scaling and training. So, it is important not to commit to one type of experiment, and instead invest in experiment agnostic search.</p>

                    <h2>General Information-Theoretic Semantics</h2>
                    <h3>Non-Experiment Objects</h3>
                    <p>Experiments are not the only type of information that occupies this symbolic manifold \(\mathcal{S}\). Any thing distinct from nothingness and from other pieces of information may inhabit the space. For example, a dog, a person, a house, a feeling, an imageâ€”all these different human comprehensible objects have a unique essence that presents differently depending on the language used. This seems obvious because a single object has a different name across natural languages (e.g., "dog" \(\equiv\) "perro" \(\equiv\) "hund").</p>

                    <h3>Alternate Representations and Semantic Geometry</h3>
                    <p>It is natural to think that new languages might exist that allow us to express these symbolic essences more succinctly, or traverse between them with greater ease. So far, the semantic space of real-valued vectors \(\mathbb{R}^n\) is the most tractable language for expressing objects. We know, for example, that adding the vector difference to a new origin yields semantic consistency:</p>
                    
                    <!-- Block Math Equation -->
                    <p>$$ \vec{v}_{\text{king}} - \vec{v}_{\text{man}} + \vec{v}_{\text{woman}} \approx \vec{v}_{\text{queen}} $$</p>
                    
                    <p>So, an abstract space's geometry or rules may encode its semantics.</p>

                    <h3>Traversals</h3>
                    <p>This proposition raises the issue of traversals. If two experiments \(e_1\) and \(e_2\) are static "snapshots," for example an LLM's parameters \(\theta_t\) and \(\theta_{t+1}\), then moving from \(e_1\) to \(e_2\) is precisely what happens during training. More generally, traversing the semantic vector space traces a path \(\gamma(t)\), and an onlooker may assign different values to each path. For example, an experimentalist may deem a path quickly converging to the global loss minimum \(\mathcal{L}_{min}\) as "good."</p>
                    
                    <figure class="my-8 p-8 bg-gray-50 border border-gray-200 rounded-lg text-center flex flex-col items-center justify-center">
                    <img
                        src="semantic_trajectory.png"
                        alt="A trajectory Î³(t) between eâ‚ and eâ‚‚ in a 3D semantic coordinate system"
                        class="max-h-64 w-auto mb-4"
                    >
                    <figcaption class="mt-2 text-sm text-gray-400 italic">
                        Fig 1. A visualization of a trajectory \(\gamma(t)\) through the semantic vector space.
                    </figcaption>
                </figure>

                    <h3>Path Generality</h3>
                    <p>Since these paths are not confined to specific architectures, the path might start with an LLM, morph into a GAN or VAE, and settle at a diffusion model. Moreover, given these paths are not restricted to learning systems at all, a path may derail into a non-experiment object, especially when interpolating between well-defined objects in a smooth space. This derailment happens when a language model garbles improvements to a learning experiment. If a learning model is the algorithm articulating these experiment traversals, the path represents a recursively self-improving algorithm. The focus of this article is on linguistic path descriptions rather than path properties, so the reader should refer to <a href="experiment_traversals.html" class="article-link">this blog post</a> for a fuller discussion.</p>

                    <h2>Canonical Forms and the Semantic Basis</h2>
                    
                    <h3>The Entanglement of Natural Language</h3>
                    <p>If the symbolic space really can represent all semantically distinct objects, we must address the problem of entanglement. In natural language, concepts like "Politeness" and "Formality" are often treated as distinct axes, but more accurately, they are amalgams of more fundamental semantic primitives. These semantic primitives may be incomprehensible to us, just as the binary representation of a song or the RGB values of an image cannot individually convey their equivalent semantic composition.</p>
                    <p>Nonetheless, this semantic decomposition is mathematically useful. For example, by varying a subset of these bases, one fixes useful semantic properties while optimising over variables. Concretely, let \(x_1, x_2, x_3\) be the primitives comprising "interpretability", then let \(x_4, x_5, x_6\) compose to "performance". Fixing the first three bases while varying the second three will change peformance while retaining interpretability, and thus with sufficiently primitive bases, paths can preserve useful symmetries.</p> 
                    <p>Returning to the manifold and fiber bundles, movement along the fiber of a desirable symmetry can optimise a representation while retaining a useful property. For example, when traversing the canonical manifold, a researcher may first settle on a highly interpretable model and then move around its fiber to change the model's representation to a functionally equivalent architecture. So, there is scope for preserving symmetries within and between representations.</p>

                    <h3>The Canonical, Decomposed Space</h3>
                    <p>If we can derive the true canonical bases of the semantic vector space (SVS), we may dispense with the complexity of fibers entirely. Instead of accepting the manifold hypothesis' canonical non-Euclidean space, we can take the infinite-dimensional Euclidean interpretation as fundamental. Then, the fiber bundles that map to different reprentations are flattened within the semantic space. For example, bases \(x_7, x_8, x_9\) might specify the current reprentation, allowing us to flatten the fibre bundles into a single Euclidean vector space. It is slightly paradoxical to think of the SVS representation as at once specifying the current representation, while not being the representation. But, this problem exists whenever a canonical space of forms is presumed; the canonical space at once prescribes a representation, while not being the representation.<\p>
                    <p>It must be emphasised that this flattened "One Space" model, one need not traverse a curved fiber to preserve symmetry;simply mask or traverse a specific subset of the atomic axes.</p>

                    <h2>Recursive Geometry</h2>
                    <h3>Models and Paths as Coordinates</h3>
                    <p>An important implication of the canonical SVS is the recursion arising for the models themselves. Typically, a Large Language Model (LLM) is a function \(f(x)\) that processes vectors. However, within the SVS of "experiments," the LLM itself, its architecture, parameters, and loss function, occupies a single coordinate point \(P_{LLM} \in \mathbb{R}^N\). It is a static object within the meta-space of possible intelligences. N.B.: The SVS's completeness further implies that there exists a point for each path through the space. <\p>
                    
                    <h3>Linear Trajectories to AGI</h3>
                    <p>These facts imply the "Path Generality" mentioned is literally a trajectory optimisation problem. By experessing preferences over time-paths a person can vary trajectories through the space of architectures such that at any moment the path specifies a task-optimal architecture. Further, if the space is truly canonical and flattened via the correct basis decomposition, the path from an imperfect model to a target state (AGI or an optimal predictor) need not be a complex, non-convex search. Instead, the bases may render the optimal path linear (not curved; or a simple geodesic in the manifold case). It is then interesting to imagine whether these optimal paths are recoverable similarly to parameter-optimising paths. In this way, we may test the general applicability of an optimisation method like gradient descent.</p>
                </div>
            <section class="footnotes mt-10 pt-6 border-t border-gray-200">
                <h3 class="text-base font-semibold text-gray-700 mb-3">Notes</h3>
                <ol class="list-decimal pl-6 text-sm text-gray-500 space-y-2">
                    <li id="fn1">
                        Perhaps more technically, if the number of representations varies across objects,
                        the space is a diffeological space of sheaves.
                        <a href="#fnref1" class="article-link ml-1">â†©ï¸Ž</a>
                    </li>
                </ol>
            </section>
            </div>
        </article>
    </main>

    <footer class="bg-white border-t border-gray-200 mt-12">
        <div class="container mx-auto px-6 py-8 text-center">
            <div class="flex justify-center space-x-8 mb-4">
                <a id="footer-email-link" href="#" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">Email</a>
                <a id="footer-linkedin-link" href="#" target="_blank" rel="noopener noreferrer" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">LinkedIn</a>
            </div>
            <p class="text-gray-400 text-sm">&copy; 2025 Pascal Tohouri. All Rights Reserved.</p>
        </div>
    </footer>

    <script>
        window.addEventListener('DOMContentLoaded', () => {
            // --- KaTeX Initialization ---
            if (window.renderMathInElement) {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true}
                    ],
                    throwOnError: false
                });
            }

            // --- Obfuscated Contact Info ---
            const emailUser = 'pascal.tohouri1';
            const emailDomain = 'gmail.com';
            const linkedinUrl = 'https://www.linkedin.com/in/pascal-tohouri/';
            const fullEmail = `mailto:${emailUser}@${emailDomain}`;
            
            const footerEmailLink = document.getElementById('footer-email-link');
            const footerLinkedinLink = document.getElementById('footer-linkedin-link');
            if(footerEmailLink) footerEmailLink.href = fullEmail;
            if(footerLinkedinLink) footerLinkedinLink.href = linkedinUrl;
        });
    </script>
</body>
</html>
