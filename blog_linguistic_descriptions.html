<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="An essay by Pascal Tohouri on machine learning experiment traversals, and formal languages.">
    <title>Describing Experiment Traversals - Pascal Tohouri</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- KaTeX CSS and JS for Math Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- Inter Font to match Home Page -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">

    <style>
        /* Typography Settings */
        body {
            font-family: 'Inter', sans-serif;
            -webkit-font-smoothing: antialiased;
        }

        /* Headers and UI Fonts */
        h1, h2, h3, h4 {
            font-family: 'Inter', sans-serif;
            letter-spacing: -0.015em;
        }
        
        nav, footer, .meta-data {
            font-family: 'Inter', sans-serif;
        }

        /* LaTeX-style Numbering System */
        .article-body {
            counter-reset: section;
        }
        .article-body p {
            margin-bottom: 1.15rem;
            line-height: 1.7; /* Slightly tighter line height for academic feel */
            font-size: 0.95rem; /* Reduced size for compact academic look */
            color: #374151;
            text-align: justify;
            text-justify: inter-word;
        }

        .article-body h2 {
            font-size: 1.25rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            line-height: 1.3;
            color: #111827; /* Gray 900 */
            counter-reset: subsection;
        }

        .article-body h2::before {
            counter-increment: section;
            content: counter(section) ". ";
        }

        .article-body h3 {
            font-size: 1.05rem;
            font-weight: 600;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            line-height: 1.3;
            color: #374151; /* Gray 700 */
            counter-reset: subsubsection;
        }

        .article-body h3::before {
            counter-increment: subsection;
            content: counter(section) "." counter(subsection) " ";
        }

        /* Link Styling */
        .article-link {
            color: #2563eb;
            text-decoration: underline;
            text-decoration-thickness: 1px;
            text-underline-offset: 2px;
        }
        .article-link:hover {
            color: #1e40af;
        }
    </style>
</head>
<body class="bg-stone-50 text-stone-800">

    <header class="bg-white shadow-sm sticky top-0 z-50 border-b border-gray-100">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center max-w-4xl">
            <a href="index.html" class="text-base font-semibold tracking-tight text-stone-900">Pascal Tohouri</a>
            <a href="index.html" class="text-xs font-medium text-stone-500 hover:text-stone-900 transition duration-300 group">
                <span class="inline-block transition-transform group-hover:-translate-x-1">&larr;</span> Back to Home
            </a>
        </nav>
    </header>

    <main>
        <article class="container mx-auto px-4 py-12 md:py-16 max-w-3xl">
            <div class="bg-white shadow-sm rounded-xl px-6 md:px-12 py-10 border border-gray-100">
                
                <header class="mb-8 text-center">
                    <h1 class="text-2xl md:text-3xl font-semibold text-stone-900 leading-tight mb-3 tracking-tight">Describing Experiment Traversals</h1>
                    <div class="meta-data flex justify-center gap-4 text-xs text-stone-500 uppercase tracking-wider font-medium">
                        <span>Nov 18, 2025</span>
                        <span>&bull;</span>
                        <span>5 Min Read</span>
                    </div>
                </header>

                <div class="article-body">
                    <h2>Learning Representations and Symbolic Information</h2>
                    <h3>Experiment Features</h3>
                    <p>A machine learning experiment comprises many different features. 
                        For example, when training a Large Language Model (LLM), 
                        one specifies the architecture (attention heads, hidden dimension size, normalisation strategy...), 
                        loss function, learning rate \(\eta\), etc.. These elements make the experiment unique; 
                        changing them would recover a different experiment \(e'\).
                    </p>
                    
                    <h3>Differing Representations</h3>
                    <p>Each experiment's symbolic information is like an abstract symmetry preserved across different representations. 
                        For example, one might represent an experiment (where an LLM is trained via SGD) as a programming script, 
                        a semantic vector \(\vec{v}_e\), or in natural language. 
                        Each representation has different linguistic rules, and thus different properties 
                        (e.g., completeness, description lengths, self-referencing), but translations preserve the essence of an experiment. 
                        It is therefore assumed here that an experiment is an equivalence class of 
                        representations that preserve task-relevant structure.
                    </p>
                    
                    <h3>Platonic and Structuralist Forms</h3>
                    <p>In the Platonic view, each experiment's abstraction resides in the canonical space of forms; 
                        a symbolic space of semantically distinct objects. In the structuralist view, 
                        no independent space of forms exists; an experiment is just the enduring symmetries that translate between the experiment's representations. 
                        One unifying interpretation is the manifold hypothesis, which unites these views by supposing a symbolic space 
                        \(\mathcal{S}\) where each semantic object sits on a manifold and has an associated fiber containing all possible representations. 
                        So, the symbolic representation of "LLM" has a mapping to each representation including "programming script", 
                        "technical report", "semantic vector", and so forth.
                    </p>
                    
                    <p>Loosely speaking, a manifold of fiber bundles is like a Moduli space<sup class="footnote-ref"><a href="#fn1" id="fnref1">1</a></sup>. 
                        I'm using this analogy informally and it is already somewhat problematic because it presupposes a lot of structure onto the canonical space of forms. 
                        For example, it is intuitively disquieting that a fundamental, abstract space should happen to be locally Euclidean. 
                        Immediately therefore, it is easy to acquiesce to the structarlist view of persistent symmtric properties, 
                        treating this canonical space as just another representation, not a literal description of reality. 
                        Still, the manifold hypothesis is useful here for illustrative purposes.
                    </p>
                    
                    <h3>Method-Specific Limitations</h3>
                    <p>A task with well-defined preferences over all these experiments may have an optimum \(e^*\), 
                        but holding some experiment features fixed, the researcher may never find it. 
                        For example, sticking to an LLM architecture may guarantee another superior architecture is never discovered, 
                        irrespective of the gains from scaling and training. So, it is important not to commit to one type of experiment, 
                        and instead invest in experiment agnostic search.
                    </p>

                    <h2>General Information-Theoretic Semantics</h2>
                    <h3>Non-Experiment Objects</h3>
                    <p>Experiments are not the only type of information that occupies this symbolic manifold \(\mathcal{S}\). 
                        Any thing distinct from nothingness and from other pieces of information may inhabit the space. 
                        For example, a dog, a person, a house, a feeling, an image 
                        - all these different human comprehensible objects have a unique essence that presents differently depending on the language used. 
                        This seems obvious because a single object has a different name across natural languages 
                        (e.g., "dog" \(\equiv\) "perro" \(\equiv\) "hund").
                    </p>

                    <h3>Alternate Representations and Semantic Geometry</h3>
                    <p>It is natural to think new languages might exist that allow us to express these symbolic essences more succinctly, 
                        or traverse between them with greater ease. So far, the semantic space of real-valued vectors \(\mathbb{R}^n\) is the most tractable language for expressing objects. 
                        We know, for example, that adding the vector difference to a new origin yields semantic consistency:
                    </p>
                    
                    <!-- Block Math Equation -->
                    <p>$$ \vec{v}_{\text{king}} - \vec{v}_{\text{man}} + \vec{v}_{\text{woman}} \approx \vec{v}_{\text{queen}} $$</p>
                    
                    <p> So, an abstract space's geometry or rules may encode its semantics. This is exactly the sort of structure exploited by word-embedding models like 
                        <a href="https://arxiv.org/abs/1301.3781" class="article-link">Word2Vec</a> and <a href="https://aclanthology.org/D14-1162/" class="article-link">GloVe</a>, 
                        where linear relations in \(\mathbb{R}^n\) often correlate with semantic analogies. 
                        (I quietly think there is a more principled language with the same tractability as the SVS for expressing these movements, 
                        similar to the approach of 
                        <a href="https://www.researchgate.net/publication/45907781_Mathematical_Foundations_for_a_Compositional_Distributional_Model_of_Meaning#:~:text=framework.%20...-,...,natural%20language%20processing.%20..." class="article-link">
                        Coecke et al.</a>.)
                    </p>

                    <h3>Traversals</h3>
                    <p>This proposition raises the issue of traversals. 
                       Suppose two experiments \(e_{t}\) and \(e_{t+1}\) are static "snapshots," and that these experiments are identical except for an LLM's parameter vector \(\theta_t\) and \(\theta_{t+1}\), 
                        then moving from \(e_t\) to \(e_{t+1}\) is precisely what happens during training. More generally, traversing the semantic vector space traces a path \(\gamma(t)\), 
                        and an onlooker may assign different values to each path. 
                        For example, an experimentalist may deem a path quickly converging to the global loss minimum \(\mathcal{L}_{min}\) as "good."
                    </p>
                    
                    <figure class="my-8 p-8 bg-gray-50 border border-gray-200 rounded-lg text-center flex flex-col items-center justify-center">
                    <!-- Placeholder used since original image was not provided -->
                    <img
                        src="semantic_trajectory.png"
                        alt="Trajectory γ(t) between e₁ and e₂ in a semantic coordinate system"
                        class="max-h-64 w-auto mb-4"
                    />
                    <figcaption class="mt-2 text-sm text-gray-400 italic">
                        Fig 1. A visualization of a trajectory \(\gamma(t)\) through the semantic vector space.
                    </figcaption>
                </figure>

                    <h3>Path Generality</h3>
                    <p>Since these paths are not confined to specific architectures, the path might start with an LLM, morph into a GAN or VAE, and settle at a diffusion model. 
                        Moreover, given these paths are not restricted to learning systems at all, a path may derail into a non-experiment object, 
                        especially when interpolating between well-defined objects in a smooth space. This derailment happens when a language model garbles improvements to a learning experiment. 
                        In the special case where a learning modell articulates these experiment traversals, the path represents a recursively self-improving algorithm. 
                        Recent JEPA work by <a href="information_theory_ml" class="article-link">Huang et al.</a> is approaching semantic vector traversal by applying the loss to the encoded vector, not the output.
                        The focus of this article is on linguistic path descriptions, so the reader should refer to <a href="information_theory_ml" class="article-link">this blog post</a> 
                        for a fuller discussion.
                    </p>

                    <h2>Canonical Forms and the Semantic Basis</h2>
                    
                    <h3>The Entanglement of Natural Language</h3>
                    <p>If the symbolic space really can represent all semantically distinct objects, we must address the problem of entanglement. 
                        In natural language, concepts like "Politeness" and "Formality" are often treated as distinct axes, but more accurately, 
                        they are amalgams of more fundamental semantic primitives. These semantic primitives may be incomprehensible to us, 
                        just as the binary representation of a song or the RGB values of an image cannot individually convey their equivalent semantic composition.
                    </p>
                        
                    <p>Nonetheless, this semantic decomposition is mathematically useful. For example, by varying a subset of these bases, 
                        one fixes useful semantic properties while optimising over variables. Concretely, let \(x_1, x_2, x_3\) be the primitives comprising "interpretability", 
                        then let \(x_4, x_5, x_6\) compose to "performance". Importantly, suppose there is no basis that contributes to both concepts, or that if a basis (say \(x_3\)) were contributing to both, 
                        that one could decompose \(x_3\) into orthogonal components (say \(x'_3, x'_4\) ) that contribute separately to each concept by composition. 
                        Fixing the first three bases while varying the second three will change peformance while retaining interpretability, and thus with sufficient decomposition, paths can preserve useful symmetries by moving along the hyper-plane of constant primitives. 
                        In practice we should not expect to literally recover these primitives; I’m using them as idealisations to consider holding interpretability fixed while improving performance.
                    </p>
                        
                    <p>Returning to the manifold and fiber bundles, movement along the fiber of a desirable symmetry can optimise a representation while retaining a useful property. 
                        For example, when traversing the canonical manifold, a researcher may first settle on a highly interpretable model and then move around its fiber to change the model's representation. 
                        So, there is scope for preserving symmetries within and between representations.
                    </p>

                    <h3>The Canonical, Decomposed Space</h3>
                    <p>If we can derive the true canonical bases of the semantic vector space (SVS), we may dispense with the complexity of fibers entirely. 
                        Instead of accepting the manifold hypothesis' canonical non-Euclidean space, we can take the infinite-dimensional Euclidean interpretation as fundamental. 
                        Then, the fiber bundles that map to different representations are flattened within the semantic space. 
                        For example, bases \(x_7, x_8, x_9\) might specify the representation-specificying primitives, allowing us to flatten the fibre bundles into a single Euclidean vector space and go from "programming script", to "natural language", 
                        to "semantic vector". It is slightly unintuitive to think of the SVS representation as at once describing the present representation, while not being that representation. 
                        But, this hierarchical-referencing problem exists whenever a canonical space of forms is presumed; one language is used to describe another.
                    </p>
                        
                    <p>It must be emphasised that in this flattened "One Space" model, one need not traverse a curved fiber to preserve symmetry; simply mask or traverse a specific subset of the primitive axes.
                    </p>

                    <h2>Recursive Geometry</h2>
                    <h3>Models and Paths as Coordinates</h3>
                    <p>An important implication of the canonical SVS is the recursion arising for the models themselves. 
                        Typically, a Large Language Model (LLM) is a function \(f(x)\) that processes vectors. 
                        However, within the SVS of "experiments," the LLM itself, its architecture, parameters, and loss function, occupies a single coordinate point \(P_{LLM} \in \mathbb{R}^N\). 
                        It is a static object within the space of possible intelligences. The SVS's completeness further implies that there exists a point for each path through the space. 
                        In a semantically principled space, geometric transforms would describe these combinations, with the elements of an experiment transforming under composition to the semantic vector of the whole experiment. 
                        Likewise, one could project a time path on the space to a single point. This is the standard trick of moving to a function space: a trajectory \(\gamma(t)\) becomes a single object in a higher-level space, whose coordinates summarise properties of the whole curve. 
                        These projections and reconstructions essentially move between object descriptions at differing levels of depth.
                    </p>
                    
                    <h3>Linear Trajectories to Task Optimal Architectures</h3>
                    <p>These facts imply the "Path Generality" mentioned is literally a trajectory optimisation problem. 
                        By expressing preferences over time-paths a person could (in principle) vary trajectories through the space of architectures such that at any moment the path specifies a task-optimal architecture. 
                        In the idealised limit where the space is truly canonical and flattened via the right basis decomposition, the path from an imperfect model to a target state (an optimal predictor or experiment \(e^*\)) need not be a wild, non-convex search. 
                        Instead, the correct bases may render the optimal path linear (not curved), or a simple geodesic in the manifold case. 
                        It is then interesting to imagine that, if a linear trajectory could distinguish an infinitessimally fine semantic movement, whether standard optimisers, like hill-climbing or gradient descent might generalis beyond parameter tuning to a broader "semantic optimisation" over models and experiments.
                    </p>
                </div>
            <section class="footnotes mt-10 pt-6 border-t border-gray-200">
                <h3 class="text-base font-semibold text-gray-700 mb-3">Notes</h3>
                <ol class="list-decimal pl-6 text-sm text-gray-500 space-y-2">
                    <li id="fn1">
                        Perhaps more technically, if the number of representations varies across objects,
                        the space is a diffeological space of sheaves.
                        <a href="#fnref1" class="article-link ml-1">↩︎</a>
                    </li>
                </ol>
            </section>
            </div>
        </article>
    </main>

    <footer class="bg-white border-t border-gray-200 mt-12">
        <div class="container mx-auto px-6 py-8 text-center">
            <div class="flex justify-center space-x-8 mb-4">
                <a id="footer-email-link" href="#" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">Email</a>
                <a id="footer-linkedin-link" href="#" target="_blank" rel="noopener noreferrer" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">LinkedIn</a>
            </div>
            <p class="text-gray-400 text-sm">&copy; 2025 Pascal Tohouri. All Rights Reserved.</p>
        </div>
    </footer>

    <script>
        window.addEventListener('DOMContentLoaded', () => {
            // --- KaTeX Initialization ---
            if (window.renderMathInElement) {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true}
                    ],
                    throwOnError: false
                });
            }

            // --- Obfuscated Contact Info ---
            const emailUser = 'pascal.tohouri1';
            const emailDomain = 'gmail.com';
            const linkedinUrl = 'https://www.linkedin.com/in/pascal-tohouri/';
            const fullEmail = `mailto:${emailUser}@${emailDomain}`;
            
            const footerEmailLink = document.getElementById('footer-email-link');
            const footerLinkedinLink = document.getElementById('footer-linkedin-link');
            if(footerEmailLink) footerEmailLink.href = fullEmail;
            if(footerLinkedinLink) footerLinkedinLink.href = linkedinUrl;
        });
    </script>
</body>
</html>
