<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A blog post by Pascal Tohouri on probabilistic path traversal in the SVS language">
    <title>Introducing Uncertainty for Even More General Paths - Pascal Tohouri</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- KaTeX CSS and JS for Math Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- Inter Font -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">

    <style>
        /* Typography Settings */
        body {
            font-family: 'Inter', sans-serif;
            -webkit-font-smoothing: antialiased;
        }

        /* Headers and UI Fonts */
        h1, h2, h3, h4 {
            font-family: 'Inter', sans-serif;
            letter-spacing: -0.015em;
        }
        
        nav, footer, .meta-data {
            font-family: 'Inter', sans-serif;
        }

        /* LaTeX-style Numbering System */
        .article-body {
            counter-reset: section;
        }
        .article-body p, .article-body li {
            margin-bottom: 1.15rem;
            line-height: 1.7; /* Slightly tighter line height for academic feel */
            font-size: 0.95rem; /* Reduced size for compact academic look */
            color: #374151;
            text-align: justify;
            text-justify: inter-word;
        }

        .article-body h2 {
            font-size: 1.25rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            line-height: 1.3;
            color: #111827; /* Gray 900 */
            counter-reset: subsection;
        }

        .article-body h2::before {
            counter-increment: section;
            content: counter(section) ". ";
        }

        .article-body h3 {
            font-size: 1.05rem;
            font-weight: 600;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            line-height: 1.3;
            color: #374151; /* Gray 700 */
            counter-reset: subsubsection;
        }

        .article-body h3::before {
            counter-increment: subsection;
            content: counter(section) "." counter(subsection) " ";
        }

        .article-body h4 {
            font-size: 1.0rem;
            font-weight: 600;
            margin-top: 1.35rem;
            margin-bottom: 0.5rem;
            line-height: 1.3;
            color: #374151; /* Gray 700 */
        }

        /* Numbering for h4 is optional, added here for consistency */
        .article-body h4::before {
            counter-increment: subsubsection;
            content: counter(section) "." counter(subsection) "." counter(subsubsection) " ";
        }

        /* Link Styling */
        .article-link {
            color: #2563eb;
            text-decoration: underline;
            text-decoration-thickness: 1px;
            text-underline-offset: 2px;
            cursor: pointer;
        }
        .article-link:hover {
            color: #1e40af;
        }

        /* Footnote references */
        sup {
            font-size: 0.75em;
            vertical-align: super;
            line-height: 0;
        }
    </style>
</head>
<body class="bg-stone-50 text-stone-800">

    <header class="bg-white shadow-sm sticky top-0 z-50 border-b border-gray-100">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center max-w-4xl">
            <a href="website/index.html" class="text-base font-semibold tracking-tight text-stone-900">Pascal Tohouri</a>
            <a href="website/index.html" class="text-xs font-medium text-stone-500 hover:text-stone-900 transition duration-300 group">
                <span class="inline-block transition-transform group-hover:-translate-x-1">&larr;</span> Back to Home
            </a>
        </nav>
    </header>

    <main>
        <article class="container mx-auto px-4 py-12 md:py-16 max-w-3xl">
            <div class="bg-white shadow-sm rounded-xl px-6 md:px-12 py-10 border border-gray-100">
                
                <header class="mb-8 text-center">
                    <h1 class="text-2xl md:text-3xl font-semibold text-stone-900 leading-tight mb-3 tracking-tight">
                        Introducing Uncertainty for Even More General Paths
                    </h1>
                    <div class="meta-data flex justify-center gap-4 text-xs text-stone-500 uppercase tracking-wider font-medium">
                        <span>Dec 04, 2025</span>
                        <span>&bull;</span>
                        <span id="reading-time" data-wpm="200"></span>
                    </div>
                    <button 
                        id="tts-toggle" 
                        class="mt-4 inline-flex items-center gap-2 px-3 py-1.5 rounded-full border border-stone-300 text-xs font-medium text-stone-700 hover:bg-stone-100 transition"
                    >
                        ▶ Listen
                    </button>
                </header>

                <div class="article-body">
                    <p>
                        This blog post follows the problem established in <a href="#" class="article-link">A Quick Formal Description of Optimal Path Traversals</a> and <a href="#" class="article-link">Introducing Variable Endpoints for More General Experiment Paths</a>.
                    </p>
                    
                    <h2>Restating the Problem</h2>
                    <p>
                        In previous posts, we established:
                    </p>
                    <ul class="list-disc pl-6 mb-6 space-y-2 text-sm text-stone-700">
                        <li>Any machine learning experiment and all of its relevant features exist in the semantic vector space (SVS).</li>
                        <li>Training is a special case of more general inter-experiment path traversals,</li>
                        <li>The optimal path identifies the task-optimal experiment (architecture, parameters, reward functions, etc.).</li>
                        <li>Auto-regressing the rich information in a past path segment finds new experiments.</li>
                    </ul>

                    <p>
                        The ultimate goal is to specify a representation-agnostic path finder - one that moves between representations to traverse the symbolic experiment space frictionlessly. This model may escape language-specific constraints. For example, if a Hessian’s calculation is infeasible in the SVS, some other representation of the loss function’s curvature might solve more tractably. 
                    </p>
                    <p>
                        The model that seeks to attain these representation-agnostic paths is the <strong>ORBIT</strong> model (so-called for its relation to symmetries). In the prior autoregression post, we found a general formalism for moving from past to future experiments, but this formalism was incomplete; it did not specify how to select new points from a feasible set. Now, we will introduce probabilistic methods to address that uncertainty.
                    </p>

                    <h2>Bayesian Updating</h2>
                    
                    <h3>The Parable of Shifting Ballot Dunes</h3>
                    <p>
                        Before jumping to the formalisms, recall that our autoregressive path takes the coordinates of the past and applies some rule to plot new coordinates. Here, for fun, let’s invoke a new analogy. Suppose you stand on a flat plane and can see the glow of your footsteps illuminated behind you. You need some rule prescribing next steps based on those past steps. 
                    </p>
                    <div class="my-8 flex justify-center">
                        <div class="bg-gray-100 border border-gray-200 rounded-lg p-8 text-center text-sm text-gray-500 italic max-w-md">
                            [Figure 1: Visual representation of Bayesian probability updating on a 2D vector space, showing probability mass shifting based on path history]
                        </div>
                    </div>
                    <p>
                        In the semantic vector space, these footsteps are actually experiments, so the footsteps carry lots of information. For example, if you pass through crocodile-infested swamps, you may not wish to double back on yourself. If you pass a gold mine, perhaps you should double back!
                    </p>
                    <p>
                        Each past footstep casts ballots over potential future steps. The crocodile path might cast ballots away from your current location, out of danger. The gold mine might ballot the area surrounding the mine, enticing you towards it. At every time, every point on the plane must have some non-negative number of ballots and each new step shifts the ballots, such that your path on the plane is a walk among <em>shifting ballot dunes</em>. To optimise the experiment, you should move to coordinates on the plane where the weight of ballots weighs heaviest, and avoid those places that are sparsely balloted.
                    </p>

                    <h3>Cox’s Theorem and Bayesian Inevitability</h3>
                    <p>
                        Of course, these shifting ballot dunes are actually probability distributions, and an updating process drives their movements. In 1946, Richard Cox showed that Bayesian probability [Laplace (17–)] was the only method for updating a probability distribution that satisfied 3 common sense axioms:
                    </p>
                    
                    <div class="border-l-4 border-stone-200 pl-4 my-6">
                        <h4 class="text-sm uppercase tracking-wide text-stone-500 mb-2">1. Divisibility and Transitivity (Beliefs are Numbers)</h4>
                        <p class="mb-2"><strong>The Rule:</strong> The plausibility of a proposition is a real number.</p>
                        <p><strong>The Logic:</strong> If you believe $A$ is more likely than $B$, and $B$ is more likely than $C$, you must believe $A$ is more likely than $C$. This allows us to rank hypotheses.</p>
                        
                        <h4 class="text-sm uppercase tracking-wide text-stone-500 mb-2 mt-4">2. Common Sense (Compatibility with Negation)</h4>
                        <p class="mb-2"><strong>The Rule:</strong> If your belief in a proposition $A$ increases, your belief in $\text{not-}A$ must decrease.</p>
                        <p><strong>The Logic:</strong> You cannot become more convinced that "It will rain" without becoming less convinced that "It will not rain."</p>

                        <h4 class="text-sm uppercase tracking-wide text-stone-500 mb-2 mt-4">3. Consistency (Path Independence)</h4>
                        <p class="mb-2"><strong>The Rule:</strong> If a conclusion depends on multiple pieces of evidence, the order in which you process the evidence cannot matter.</p>
                        <p><strong>The Logic:</strong> Whether you see trace $x_1$ then $x_2$, or $x_2$ then $x_1$, the final belief state must be identical.</p>
                    </div>

                    <p>
                        To respect these common sense axioms, the method for updating a probability distribution over the experiment space should adhere to Bayes’ rule. Formally, for experimental encoding $X$, a distribution over the SVS $p(X)$ represents the probability that any given vector $X'$ is the optimal next experiment, conditioned on the evidence $E$ (our trace):
                    </p>

                    <p class="bg-stone-50 p-4 border border-stone-200 rounded-md text-center my-6 overflow-x-auto">
                         $$ p(X' \mid E) = \frac{p(E \mid X') p(X')}{p(E)} $$
                    </p>

                    <h3>The Fisher Information Metric and Natural Gradient</h3>
                    <p>
                        Returning to our parable, the "ballot dunes" do not reside on a flat Euclidean plane. The Semantic Vector Space is a manifold where distances represent information geometry, not mere coordinate distance. A small step in coordinates ($[1.0, 1.0] \to [1.1, 1.1]$) might represent a massive shift in model behavior, while a large step elsewhere might represent negligible change.
                    </p>
                    <p>
                        To navigate these dunes correctly, we cannot rely on the standard Euclidean gradient. We must utilize the <strong>Fisher Information Metric</strong> $G$, which acts as a Riemannian metric tensor describing the local curvature of the probability distribution. The steepest descent on this manifold is given by the <strong>Natural Gradient</strong>:
                    </p>
                    <p class="bg-stone-50 p-4 border border-stone-200 rounded-md text-center my-6 overflow-x-auto">
                        $$ \tilde{\nabla}\mathcal{L} = G^{-1} \nabla\mathcal{L} $$
                    </p>
                    <p>
                        By correcting the gradient with the inverse Fisher Information Matrix, we ensure that our "steps" through the experiment space are consistent with the shifting probability densities (the ballot dunes), rather than the arbitrary units of the parameters.
                    </p>

                    <h2>Local Bayesian Optimality in Autoregression</h2>
                    <p>
                        We can now merge this probabilistic view with the autoregressive engine defined in the previous post. Previously, we defined a deterministic function $f$ such that $\gamma(t+\Delta t) = f(\varphi(T_t))$. 
                    </p>
                    <p>
                        In the Bayesian formulation, the autoregressive model does not output a single point, but effectively parameterizes a posterior distribution over the SVS. The "best" next experiment is a sample drawn from this posterior predictive distribution:
                    </p>
                    <p class="bg-stone-50 p-4 border border-stone-200 rounded-md text-center my-6 overflow-x-auto">
                        $$ \gamma_{next} \sim p(\gamma \mid T_t, \text{ORBIT}) $$
                    </p>
                    <p>
                        This formulation allows the path to explore. When the distribution is sharp (low variance), the path exploits known good regions (the gold mine). When the distribution is flat (high variance/high entropy), the path explores broadly, driven by the uncertainty of the trace. This is the mechanism that provides the "steering" we lacked in the purely deterministic setting.
                    </p>

                    <h2>Implications for ORBIT</h2>
                    <p>
                        The integration of Bayesian updating into the ORBIT path-finding framework transforms it from a passive recorder of history into an active agent of discovery. The model no longer blindly extrapolates; it weighs the evidence of the past to construct a probability surface for the future.
                    </p>
                    <p>
                        By optimizing the Natural Gradient on this surface, ORBIT can efficiently traverse the SVS, ignoring parameter directions that do not yield information gain. This brings us one step closer to the ultimate goal: a system that not only represents experiments universally but navigates the space between them to find the optimal configuration with minimal friction.
                    </p>

                    <section class="footnotes mt-10 pt-6 border-t border-gray-200">
                        <h3 class="text-base font-semibold text-gray-700 mb-3">Notes</h3>
                        <ol class="list-decimal pl-6 text-sm text-gray-500 space-y-2">
                            <li id="fn1">
                                The concept of "ballot dunes" is loosely inspired by probability mass functions, where mass accumulates in regions of high likelihood.
                                <a href="#fnref1" class="article-link ml-1">↩︎</a>
                            </li>
                            <li id="fn2">
                                For a deeper dive into Information Geometry and the Fisher Information Matrix, see Amari (1998).
                                <a href="#fnref2" class="article-link ml-1">↩︎</a>
                            </li>
                        </ol>
                    </section>
                </div>
            </div>
        </article>
    </main>

    <footer class="bg-white border-t border-gray-200 mt-12">
        <div class="container mx-auto px-6 py-8 text-center">
            <div class="flex justify-center space-x-8 mb-4">
                <a id="footer-email-link" href="#" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">Email</a>
                <a id="footer-linkedin-link" href="#" target="_blank" rel="noopener noreferrer" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">LinkedIn</a>
            </div>
            <p class="text-gray-400 text-sm">&copy; 2025 Pascal Tohouri. All Rights Reserved.</p>
        </div>
    </footer>

    <script>
        window.addEventListener('DOMContentLoaded', () => {
            // --- KaTeX Initialization ---
            if (window.renderMathInElement && window.katex) {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true}
                    ],
                    throwOnError: false
                });
            } else {
                console.warn("KaTeX or renderMathInElement not loaded.");
            }

            // --- Reading time calculation ---
            const articleBody = document.querySelector('.article-body');
            const readingTimeSpan = document.getElementById('reading-time');
            
            if (articleBody && readingTimeSpan) {
                const text = articleBody.innerText || articleBody.textContent || '';
                const words = text.trim().split(/\s+/).filter(Boolean).length;
                
                const defaultWpm = 200;
                const wpmAttr = readingTimeSpan.getAttribute('data-wpm');
                const WORDS_PER_MINUTE = wpmAttr ? parseInt(wpmAttr, 10) || defaultWpm : defaultWpm;
                
                const minutes = Math.max(1, Math.round(words / WORDS_PER_MINUTE));
                readingTimeSpan.textContent = `${minutes} Min Read`;
            }

            // --- TTS Logic ---
            const ttsButton = document.getElementById('tts-toggle');

            if (ttsButton && 'speechSynthesis' in window && articleBody) {
                window.currentUtterance = null;
                window.speechSynthesis.cancel();

                ttsButton.addEventListener('click', () => {
                    if (window.speechSynthesis.speaking) {
                        window.speechSynthesis.cancel();
                        ttsButton.textContent = '▶ Listen';
                    } else {
                        window.speechSynthesis.cancel();
                        const text = articleBody.innerText || articleBody.textContent || '';
                        if (!text.trim()) return;

                        window.currentUtterance = new SpeechSynthesisUtterance(text);
                        window.currentUtterance.rate = 0.95;

                        window.currentUtterance.onend = () => {
                            ttsButton.textContent = '▶ Listen';
                        };

                        window.currentUtterance.onerror = (e) => {
                            console.error('Speech error:', e);
                            ttsButton.textContent = '▶ Listen';
                        };

                        ttsButton.textContent = '■ Stop';
                        window.speechSynthesis.speak(window.currentUtterance);
                    }
                });
            } else if (ttsButton) {
                ttsButton.style.display = 'none';
            }

            // --- Contact Info ---
            const emailUser = 'pascaltohouri';
            const emailDomain = 'yahoo.com';
            const linkedinUrl = 'https://www.linkedin.com/in/pascal-tohouri/';
            const fullEmail = `mailto:${emailUser}@${emailDomain}`;
            
            const footerEmailLink = document.getElementById('footer-email-link');
            const footerLinkedinLink = document.getElementById('footer-linkedin-link');
            if(footerEmailLink) footerEmailLink.href = fullEmail;
            if(footerLinkedinLink) footerLinkedinLink.href = linkedinUrl;
        });
    </script>
</body>
</html>
