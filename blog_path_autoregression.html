<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A blog post by Pascal Tohouri framing the detailed path traversal problem in the SVS language">
    <title>Introducing Variable Endpoints for More General Experiment Paths - Pascal Tohouri</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- KaTeX CSS and JS for Math Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- Inter Font -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">

    <style>
        /* Typography Settings */
        body {
            font-family: 'Inter', sans-serif;
            -webkit-font-smoothing: antialiased;
        }

        /* Headers and UI Fonts */
        h1, h2, h3, h4 {
            font-family: 'Inter', sans-serif;
            letter-spacing: -0.015em;
        }
        
        nav, footer, .meta-data {
            font-family: 'Inter', sans-serif;
        }

        /* LaTeX-style Numbering System */
        .article-body {
            counter-reset: section;
        }
        .article-body p {
            margin-bottom: 1.15rem;
            line-height: 1.7; /* Slightly tighter line height for academic feel */
            font-size: 0.95rem; /* Reduced size for compact academic look */
            color: #374151;
            text-align: justify;
            text-justify: inter-word;
        }

        .article-body h2 {
            font-size: 1.25rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            line-height: 1.3;
            color: #111827; /* Gray 900 */
            counter-reset: subsection;
        }

        .article-body h2::before {
            counter-increment: section;
            content: counter(section) ". ";
        }

        .article-body h3 {
            font-size: 1.05rem;
            font-weight: 600;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            line-height: 1.3;
            color: #374151; /* Gray 700 */
            counter-reset: subsubsection;
        }

        .article-body h3::before {
            counter-increment: subsection;
            content: counter(section) "." counter(subsection) " ";
        }

        .article-body h4 {
            font-size: 1.0rem;
            font-weight: 600;
            margin-top: 1.35rem;
            margin-bottom: 0.5rem;
            line-height: 1.3;
            color: #374151; /* Gray 700 */
        }

        .article-body h4::before {
            counter-increment: subsubsection;
            content: counter(section) "." counter(subsection) "." counter(subsubsection) " ";
        }

        /* Link Styling */
        .article-link {
            color: #2563eb;
            text-decoration: underline;
            text-decoration-thickness: 1px;
            text-underline-offset: 2px;
            cursor: pointer;
        }
        .article-link:hover {
            color: #1e40af;
        }

        /* Footnote references */
        sup {
            font-size: 0.75em;
            vertical-align: super;
            line-height: 0;
        }
    </style>
</head>
<body class="bg-stone-50 text-stone-800">

    <header class="bg-white shadow-sm sticky top-0 z-50 border-b border-gray-100">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center max-w-4xl">
            <a href="website/index.html" class="text-base font-semibold tracking-tight text-stone-900">Pascal Tohouri</a>
            <a href="website/index.html" class="text-xs font-medium text-stone-500 hover:text-stone-900 transition duration-300 group">
                <span class="inline-block transition-transform group-hover:-translate-x-1">&larr;</span> Back to Home
            </a>
        </nav>
    </header>

    <main>
        <article class="container mx-auto px-4 py-12 md:py-16 max-w-3xl">
            <div class="bg-white shadow-sm rounded-xl px-6 md:px-12 py-10 border border-gray-100">
                
                <header class="mb-8 text-center">
                    <h1 class="text-2xl md:text-3xl font-semibold text-stone-900 leading-tight mb-3 tracking-tight">
                        Introducing Variable Endpoints for More General Experiment Paths
                    </h1>
                    <div class="meta-data flex justify-center gap-4 text-xs text-stone-500 uppercase tracking-wider font-medium">
                        <span>Nov 27, 2025</span>
                        <span>&bull;</span>
                        <span id="reading-time" data-wpm="200"></span>
                    </div>
                    <button
                    id="tts-toggle"
                    class="mt-4 inline-flex items-center gap-2 px-3 py-1.5 rounded-full border border-stone-300 text-xs font-medium text-stone-700 hover:bg-stone-100 transition"
                >
                    ▶ Listen
                </button>
                </header>

                <div class="article-body">
                    <p>
                    <em>This blog post follows the problem established in <a href="#" class="article-link">A Quick Formal Description of Optimal Path Traversals</a>.</em>
                    </p>
                    
                    <h2>Restating the Problem</h2>
                    <p>
                        In the preceding articles, every machine learning experiment was described by a path $\gamma$ through the semantic vector space $\mathbb{R}^n$. 
                        We formalised this as:
                    </p>
                    <p class="bg-stone-50 p-4 border border-stone-200 rounded-md text-center my-6 overflow-x-auto">
                        $$ \gamma: [a, b] \to \mathbb{R}^n, \quad \text{where } \gamma(a) = \text{Start}, \ \gamma(b) = \text{End} $$
                    </p>
                    <p>
                        This formalism gave us a clear topological view of experiments and a route to optimisation under strict assumptions. 
                        Now, we must relax those assumptions. 
                        A good assumption to start with is the fixed endpoint assumption, because its removal is the most well-covered in the literature. 
                        Relaxing the welfare function assumptions is a more delicate thing, and I'll wait until future posts to tackle that relaxation. 
                    </p>
                    <p>
                        In the mathematics of the calculus of variations, the endpoint $\gamma(b)$ is a fixed boundary condition. 
                        In the messy real world of machine learning research, $\gamma(b)$ is the unknown optimal model we wish to discover.
                    </p>

                    <h2>Relaxing the Fixed Endpoint Assumption</h2>
                    <h3>Degrees of Uncertainty</h3>
                    <h4>Distinguishing information</h4>
                    <p>
                        At this juncture, it helps to briefly talk about the differences between a "known" and "unknown" thing.
                        Generally, most objects have many distinguishing features. For example, a person has a name, height, hair colour, and so on. 
                        These pieces of information help to distinguish two otherwise similar objects 
                        (e.g., twin A is not twin B, because twin A is 175cm tall and twin B is 178cm tall).
                        Likewise, an experiment's semantic vector encodes lots of distinguishing information.<sup id="fnref1"><a href="#fn1" class="article-link">[1]</a></sup>
                        So, if we suddenly lose one piece of information, we can no longer distinguish that experiment from all other experiments sharing the same information.
                    </p>  
                    <h4>Simple Numeric Example</h4>
                    <p>
                        For example, if experiment A's encoding is $[8,1,5]$, and we lose the final coordinate to get $[8,1,?]$, 
                        then we cannot distinguish between the class of experiments whose first two encoding coordinates are $8$ and $1$.
                        In the semantic vector space (SVS), we can walk along the line parallel to the Z-axis containing points of the form $[8,1,x]$ 
                        (e.g., $[8,1,0]$, $[8,1,4.11]$, $[8,1,15],\dots$).
                        As mentioned in a prior blog post on <a href="#" class="article-link">experiment path descriptions</a>, 
                        in a well-structured "canonical" vector space, these symmetric experiments all have similar properties.
                        The experiments are distinguished only by their third component. 
                        If this distinction is human interpretable we might find, for example,
                        that each experiment has the same model architecture but a different 
                        set of parameters.
                        These examples show there are degrees of uncertainty, and that  
                        we may not lose all information about an experiment at once.
                    </p>
                    <h4>Uncertain Regions</h4>
                    <p>
                        Generally, we can continue in this way, removing identifying pieces of information, 
                        so that the space of equivalent/symmetric points broadens from a line, to a plane, to a cubic volume, 
                        and so on. In the limit, where we know no identifying coordinate information about our "best point"; 
                        all points in the SVS are equally feasible. 
                    </p>
                    <h3>Preferred Points</h3>
                    <p>
                        While all points in the SVS may be <em>feasible</em> under complete coordinate uncertainty, 
                        it is unlikely that every point is an equally good option. 
                        For example, if all prior experiments in the path contained neural layers for compressing data, 
                        and if this compression correlated with higher performance, 
                        then one might expect subsequent experiments to evolve 
                        these compression layers.
                        The "best experiment" might then encode <en>optimised</em> neural compression. 
                    </p>
                    <p>
                    One would not, however, expect the best experiment to be the semantic vector for "kelp". 
                    This change would be unacceptably disconnected from the evidence of prior experiments. 
                    So, when we lack information about the next step, 
                    we can look back along the path to separate likely from unlikely future progressions.
                    </p>
                    <h2>Autoregression and Path sufficiency</h2>
                    <p>
                    Using past-values to specify future values is called auto-regression.
                    If a path really encodes <em>all</em> relevant information from the 
                    experiment's history, then the path should be <em>sufficient</em> for inferring 
                    or deducing subsequent member points. This claim also touches on the idea of
                    Markov processes, where one piece of information is sufficient for predicting the next. 
                    I liken these paths to jungle expeditions, where explorers use the topography and flora 
                    of their current path to orienteer their next steps.
                    </p>
                    <p>
                    To formalise, take the time-indexed experiment path:
                    $$\gamma:[a,b]\to\mathbb{R}^n,\qquad \gamma(t)\in\mathbb{R}^n.$$
                    For any time $t\in[a,b)$ denote the <em>trace</em> as the preceding points up to time $t$. 
                    If the trace is to accurately represent the path's history, 
                    it must contain all points in their order of visitation. 
                    The object to capture this ordering of (uncountably many) points is just the prior path segment: 
                    $$T_t \;=\; \gamma([a,t]) \;=\; \{\gamma(s): a\le s\le t\}.$$
                    For a time horizon beyond the present $\Delta>0$ future increments or predictions are $\gamma(t+\Delta)$. 
                    While the trace represents the ground truth, its storage and calculation may be computationally 
                    infeasible (in the infinite dim. case), 
                    and unnecessary if a compression of the trace is a sufficient summary:
                    </p>
                    <p><strong>Definition 1 (Sufficient Path Summary).</strong>
                        A path-summary map $\varphi: \mathcal{P}(\mathbb{R}^n) \to \mathcal{S}$ is  
                        sufficient with respect to the trace $T_t$ if, 
                        for every measurable set $A \subset \mathbb{R}^n$,
                        $$\mathbb{P}\big(\gamma(t+\Delta)\in A\mid T_t\big) \;=\; \mathbb{P}\big(\gamma(t+\Delta)\in A\mid \varphi(T_t)\big) \quad \text{(a.s.)}.$$
                        Equivalently, the summary $\varphi(T_t)$ 
                        screens off the full path history $T_t$ from the future.
                    </p>
                    <p><strong>Definition 2 (Minimal Sufficient Summary).</strong>
                        A summary $\varphi$ is minimal sufficient if any other summary 
                        $\psi$ satisfying the same conditional equality factors through 
                        $\varphi$ (i.e. $\varphi=\eta\circ\psi$ for some measurable $\eta$). 
                        When such a finite-dimensional $\varphi$ exists we may treat the path 
                        as having an effective finite memory.
                    </p>
                    <p>
                    Minimal sufficiency simply says a set of distinct path summaries may exist for the same trace.
                    The minimal sufficient summary is the one that minimises some measure of complexity.
                    Any other summary must map to that minimal summary via a function $\eta$.
                    These definitions allow us to import some commonly used modelling machinery. 
                    </p>                    
                    <p><strong>Order-$k$ Markov Approximation</strong> (discrete time): choose
                        $$\varphi(T_t) = \big(\gamma(t),\gamma(t-\Delta t),\dots,\gamma(t-(k-1)\Delta t)\big)\in\mathbb{R}^{nk},$$
                        as the trace summary, then let the path generate itself from its history (generative modelling):
                        $$\gamma(t+\Delta t) = f\big(\varphi(T_t)\big) + \varepsilon_{t+\Delta t},$$
                        where $\varepsilon$ is a noise term independent of $T_t$. 
                        The continuous model variant is an ordinary differential equation:
                    </p>
                    <p><strong>Continuous Autoregressive Form:</strong>
                        $$\dot\gamma(t) = F\big(\varphi(T_t),t\big) + \xi(t),$$
                        with $\xi$ a stochastic perturbation. 
                        Here $\varphi$ can encode short-term history or features of the whole past. 
                        The discrete and continuous models are an engine that drive us through the space of all experiments. 
                        In the discrete case, we take steps and in the continuous case, the path is smooth. 
                        By letting this engine run, we can join a path segment to its next element. 
                    </p>
                    <h2>Conclusion: Implications for ORBIT</h2>
                    <p>
                        If one repeats the autoregressive process, 
                        one can continually elongate a path to produce perpetual motion in the space of experiments.
                        This experiment engine is what we'll use to jump between experimental states during active learning.
                        So far, however, the engine has no steering; we cannot direct an experiment to its "best state".
                        In the next blog post, we will introduce the probabilistic techniques that optimally steer an experiment through the SVS.
                    </p>


            <section class="footnotes mt-10 pt-6 border-t border-gray-200">
                <h3 class="text-base font-semibold text-gray-700 mb-3">Notes</h3>
                <ol class="list-decimal pl-6 text-sm text-gray-500 space-y-2">
                    <li id="fn1">
                        It is interesting to imagine baking time into an SVS encoding. Then, the path parameter would be a real number that could index a path that retrogresses through time. 
                        In reality, though, experiments proceed unidirectionally in time, so encoding time into a semantic vector path may be arbitrary if time increases in a 1-to-1 relation with the path parameter.
                        If, however, (i) time progresses differently for two experiments, or (ii) the concept of time is entangled meaningfully with other bases, then its inclusion in the SVS encoding is necessary to comprehensibly describe an experiment.
                        <a href="#fnref1" class="article-link ml-1">↩︎</a>
                    </li>
                    <li id="fn2">
                        If experiment paths are sufficiently detailed, 
                        encoding information like the temperature of a GPU, 
                        or the cognitive state of a researcher, 
                        these paths might tend towards the literal progression of objects through spacetime. 
                        One must consider, therefore, 
                        that a path is constrained and driven by the physical laws of nature.
                        <a href="#fnref2" class="article-link ml-1">↩︎</a>
                    </li>
                </ol>
            </section>
            </div>
        </article>
    </main>

    <footer class="bg-white border-t border-gray-200 mt-12">
        <div class="container mx-auto px-6 py-8 text-center">
            <div class="flex justify-center space-x-8 mb-4">
                <a id="footer-email-link" href="#" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">Email</a>
                <a id="footer-linkedin-link" href="#" target="_blank" rel="noopener noreferrer" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">LinkedIn</a>
            </div>
            <p class="text-gray-400 text-sm">&copy; 2025 Pascal Tohouri. All Rights Reserved.</p>
        </div>
    </footer>

    <script>
        window.addEventListener('DOMContentLoaded', () => {
            // --- KaTeX Initialization ---
            if (window.renderMathInElement && window.katex) {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true}
                    ],
                    throwOnError: false
                });
            } else {
                console.warn("KaTeX or renderMathInElement not loaded.");
            }

            // --- Reading time calculation ---
            const articleBody = document.querySelector('.article-body');
            const readingTimeSpan = document.getElementById('reading-time');
            
            if (articleBody && readingTimeSpan) {
                const text = articleBody.innerText || articleBody.textContent || '';
                const words = text.trim().split(/\s+/).filter(Boolean).length;
                
                const defaultWpm = 200;
                const wpmAttr = readingTimeSpan.getAttribute('data-wpm');
                const WORDS_PER_MINUTE = wpmAttr ? parseInt(wpmAttr, 10) || defaultWpm : defaultWpm;
                
                const minutes = Math.max(1, Math.round(words / WORDS_PER_MINUTE));
                readingTimeSpan.textContent = `${minutes} Min Read`;
            }

            // --- FIXED Text-to-speech reader ---
            const ttsButton = document.getElementById('tts-toggle');

            if (ttsButton && 'speechSynthesis' in window && articleBody) {
                
                // 1. We attach the utterance to 'window' to prevent Garbage Collection on PC
                window.currentUtterance = null;

                // 2. Clear any "stuck" audio when the page loads
                window.speechSynthesis.cancel();

                ttsButton.addEventListener('click', () => {
                    // Check if the browser is currently speaking
                    if (window.speechSynthesis.speaking) {
                        // STOP ACTION
                        window.speechSynthesis.cancel();
                        ttsButton.textContent = '▶ Listen';
                    } else {
                        // PLAY ACTION
                        
                        // Always cancel before starting to clear the queue
                        window.speechSynthesis.cancel();

                        const text = articleBody.innerText || articleBody.textContent || '';
                        if (!text.trim()) return;

                        // Create the utterance and store it GLOBALLY
                        window.currentUtterance = new SpeechSynthesisUtterance(text);
                        window.currentUtterance.rate = 0.95;

                        // Handle the end of speech
                        window.currentUtterance.onend = () => {
                            ttsButton.textContent = '▶ Listen';
                        };

                        // Error handling (helps debug on PC)
                        window.currentUtterance.onerror = (e) => {
                            console.error('Speech error:', e);
                            ttsButton.textContent = '▶ Listen';
                        };

                        ttsButton.textContent = '■ Stop';
                        window.speechSynthesis.speak(window.currentUtterance);
                    }
                });
            } else if (ttsButton) {
                ttsButton.style.display = 'none';
            }

            // --- Obfuscated Contact Info ---
            const emailUser = 'pascaltohouri';
            const emailDomain = 'yahoo.com';
            const linkedinUrl = 'https://www.linkedin.com/in/pascal-tohouri/';
            const fullEmail = `mailto:${pascaltohouri}@${yahoo.com}`;
            
            const footerEmailLink = document.getElementById('footer-email-link');
            const footerLinkedinLink = document.getElementById('footer-linkedin-link');
            if(footerEmailLink) footerEmailLink.href = fullEmail;
            if(footerLinkedinLink) footerLinkedinLink.href = linkedinUrl;
        });
    </script>
</body>
</html>
