<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="A blog post by Pascal Tohouri framing the detailed path traversal problem in the SVS language">
    <title>Introducing Uncertainty for More General Experiment Paths - Pascal Tohouri</title>
    
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- KaTeX CSS and JS for Math Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>

    <!-- Google Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <!-- Inter Font -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">

    <style>
        /* Typography Settings */
        body {
            font-family: 'Inter', sans-serif;
            -webkit-font-smoothing: antialiased;
        }

        /* Headers and UI Fonts */
        h1, h2, h3, h4 {
            font-family: 'Inter', sans-serif;
            letter-spacing: -0.015em;
        }
        
        nav, footer, .meta-data {
            font-family: 'Inter', sans-serif;
        }

        /* LaTeX-style Numbering System */
        .article-body {
            counter-reset: section;
        }
        .article-body p {
            margin-bottom: 1.15rem;
            line-height: 1.7; /* Slightly tighter line height for academic feel */
            font-size: 0.95rem; /* Reduced size for compact academic look */
            color: #374151;
            text-align: justify;
            text-justify: inter-word;
        }

        .article-body h2 {
            font-size: 1.25rem;
            font-weight: 600;
            margin-top: 2rem;
            margin-bottom: 0.75rem;
            line-height: 1.3;
            color: #111827; /* Gray 900 */
            counter-reset: subsection;
        }

        .article-body h2::before {
            counter-increment: section;
            content: counter(section) ". ";
        }

        .article-body h3 {
            font-size: 1.05rem;
            font-weight: 600;
            margin-top: 1.5rem;
            margin-bottom: 0.5rem;
            line-height: 1.3;
            color: #374151; /* Gray 700 */
            counter-reset: subsubsection;
        }

        .article-body h3::before {
            counter-increment: subsection;
            content: counter(section) "." counter(subsection) " ";
        }

        .article-body h4 {
            font-size: 1.0rem;
            font-weight: 600;
            margin-top: 1.35rem;
            margin-bottom: 0.5rem;
            line-height: 1.3;
            color: #374151; /* Gray 700 */
        }

        .article-body h4::before {
            counter-increment: subsubsection;
            content: counter(section) "." counter(subsection) "." counter(subsubsection) " ";
        }

        /* Link Styling */
        .article-link {
            color: #2563eb;
            text-decoration: underline;
            text-decoration-thickness: 1px;
            text-underline-offset: 2px;
            cursor: pointer;
        }
        .article-link:hover {
            color: #1e40af;
        }

        /* Footnote references */
        sup {
            font-size: 0.75em;
            vertical-align: super;
            line-height: 0;
        }
    </style>
</head>
<body class="bg-stone-50 text-stone-800">

    <header class="bg-white shadow-sm sticky top-0 z-50 border-b border-gray-100">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center max-w-4xl">
            <a href="website/index.html" class="text-base font-semibold tracking-tight text-stone-900">Pascal Tohouri</a>
            <a href="website/index.html" class="text-xs font-medium text-stone-500 hover:text-stone-900 transition duration-300 group">
                <span class="inline-block transition-transform group-hover:-translate-x-1">&larr;</span> Back to Home
            </a>
        </nav>
    </header>

    <main>
        <article class="container mx-auto px-4 py-12 md:py-16 max-w-3xl">
            <div class="bg-white shadow-sm rounded-xl px-6 md:px-12 py-10 border border-gray-100">
                
                <header class="mb-8 text-center">
                    <h1 class="text-2xl md:text-3xl font-semibold text-stone-900 leading-tight mb-3 tracking-tight">
                        Introducing Uncertainty for More General Experiment Paths
                    </h1>
                    <div class="meta-data flex justify-center gap-4 text-xs text-stone-500 uppercase tracking-wider font-medium">
                        <span>Nov 27, 2025</span>
                        <span>&bull;</span>
                        <span id="reading-time" data-wpm="200"></span>
                    </div>
                    <button
                    id="tts-toggle"
                    class="mt-4 inline-flex items-center gap-2 px-3 py-1.5 rounded-full border border-stone-300 text-xs font-medium text-stone-700 hover:bg-stone-100 transition"
                >
                    ▶ Listen
                </button>
                </header>

                <div class="article-body">
                    <p>
                    <em>This blog post follows the problem established in <a href="#" class="article-link">A Quick Formal Description of Optimal Path Traversals</a>.</em>
                    </p>
                    
                    <h2>Restating the Problem</h2>
                    <p>
                        In the preceding articles, every machine learning experiment was described by a path $\gamma$ through the semantic vector space $\mathbb{R}^n$. 
                        We formalised this as:
                    </p>
                    <p class="bg-stone-50 p-4 border border-stone-200 rounded-md text-center my-6 overflow-x-auto">
                        $$ \gamma: [a, b] \to \mathbb{R}^n, \quad \text{where } \gamma(a) = \text{Start}, \ \gamma(b) = \text{End} $$
                    </p>
                    <p>
                        This formalism gave us a clear topological view of experiments and a route to optimisation under strict assumptions. 
                        Now, we must relax those assumptions. 
                        A good assumption to start with is the fixed endpoint assumption, because its removal is the most well-covered in the literature. 
                        Relaxing the welfare function assumptions is a more delicate thing, and I'll wait until the next post to tackle that relaxation. 
                    </p>
                    <p>
                        In the mathematics of the calculus of variations, the endpoint $\gamma(b)$ is a fixed boundary condition. 
                        In the messy real world of machine learning research, $\gamma(b)$ is the unknown optimal model we wish to discover.
                    </p>

                    <h2>Relaxing the Fixed Endpoint Assumption</h2>
                    <h3>Degrees of Uncertainty</h3>
                    <p>
                        At this juncture, it helps to briefly talk about the differences between a "known" and "unknown" thing.
                        Generally, most objects have many distinguishing features. For example, a person has a name, height, hair colour, and so on. 
                        These pieces of information help to distinguish two otherwise similar objects 
                        (e.g., person A is not person B, because person A is 175cm tall and person B is 178cm tall).
                        Likewise, an experiment's semantic vector encodes lots of distinguishing information.<sup id="fnref1"><a href="#fn1" class="article-link">[1]</a></sup>
                        So, if we suddenly lose one piece of information, we can no longer distinguish that experiment from all other experiments sharing the same information.
                    </p>  
                    <p>
                        For example, if experiment A's encoding is $[8,1,5]$, and we lose the final coordinate to get $[8,1,?]$, 
                        then we cannot distinguish between the class of experiments whose first two encoding coordinates are $8$ and $1$.
                        In the semantic vector space (SVS), we can walk along the line parallel to the Z-axis containing points of the form $[8,1,x]$ 
                        (e.g., $[8,1,0]$, $[8,1,4.11]$, $[8,1,15],\dots$).
                        As mentioned in a prior blog post on <a href="#" class="article-link">experiment path descriptions</a>, 
                        in a well-structured "canonical" vector space, these symmetric experiments all have similar properties.
                        The experiments are distinguished only by their third component, and if this distinction has some human interpretable form,
                        we might find, say, each experiment has the same model architecture but a different set of parameter values.
                        These examples show that there are degrees of uncertainty whereby we may not lose all the information about an experiment at once.
                    </p>
                    <p>
                        Generally, we can continue in this way, removing identifying pieces of information, 
                        so that the space of equivalent/symmetric points broadens from a line, to a plane, to a cubic volume, 
                        and so on. In the limit, where we know no identifying coordinate information about our "best point", 
                        all points in the SVS are equally feasible. 
                    </p>
                    <h3>Preferred Points</h3>
                    <p>
                        While all points in the SVS may be <em>feasible</em> under coordinate uncertainty, 
                        it is unlikely that every point is an equally attractive option. 
                        For example, if all prior experiments in the path contained neural layers for compressing data, 
                        and if this compression correlated with higher performance, 
                        then one might expect that subsequent experiments would evolve 
                        these compression layers, 
                        and that the "best experiment" would contain the pinnacle of efficient neural compression. 
                    </p>
                    <p>
                    One would not, however, expect the best experiment to be the semantic vector for "kelp". 
                    This change would be unacceptably disconnected from the evidence of prior experiments. 
                    So, when we lack information about the next step, 
                    we can look back along the path to separate likely from unlikely future progressions.
                    </p>
                    <h2>Autoregression and Path sufficiency</h2>
                    <p>
                    When an endpoint is a priori unknown but deducible from context, one can use prior experiments in a process called auto-regression.
                    If the path really encodes <em>all</em> relevant information from the 
                    experiment's history, then the path should be <em>sufficient</em> for inferring 
                    or deducing its next member points. This claim touches on the idea of autoregressive time series models 
                    and Markov processes, where one piece of information is sufficient for predicting the next. 
                    I liken these paths to jungle expeditions, where explorers use the topography and fauna 
                    of their current path to orienteer their next steps.
                    </p>
                    <p>
                    To formalise, take the time-indexed experiment path:
                    $$\gamma:[a,b]\to\mathbb{R}^n,\qquad \gamma(t)\in\mathbb{R}^n.$$
                    For any time $t\in[a,b)$ denote the <em>trace</em> (preceding points) up to time $t$ by
                    $$T_t \;=\; \gamma([a,t]) \;=\; \{\gamma(s): a\le s\le t\}.$$
                    For a time horizon beyond the present $\Delta>0$ future increments or predictions are $\gamma(t+\Delta)$. 
                    If the trace is to accurately represent the path's history, it must contain all visited points in their order of visitation.
                    The object to capture this ordering of (maybe uncountably many) points 
                    </p>

                    <p>
                    <strong>Definition 1 (Sufficiency of the trace).</strong>
                    The trace $T_t$ is <em>sufficient</em> for predicting $\gamma(t+\Delta)$ iff there exists a measurable map (a path-summary)
                    $$\varphi:\;\mathcal{P}(\mathbb{R}^n)\to\mathcal{S}$$
                    into some (typically finite-dimensional) summary space $\mathcal{S}$ such that, for every measurable set $A\subset\mathbb{R}^n$,
                    \[
                    \mathbb{P}\big(\gamma(t+\Delta)\in A\mid T_t\big)
                    \;=\;
                    \mathbb{P}\big(\gamma(t+\Delta)\in A\mid \varphi(T_t)\big)
                    \qquad\text{(a.s.)}.
                    \]
                    Equivalently, the σ-algebra generated by $T_t$ is conditionally independent of the future given the summary $\varphi(T_t)$.
                    </p>

                    <p>
                    <strong>Definition 2 Minimal sufficient summary.</strong>
                    A summary $\varphi$ is minimal sufficient if any other summary $\psi$ satisfying the same conditional equality factors through $\varphi$ (i.e. $\psi=\eta\circ\varphi$ for some measurable $\eta$). When such a finite-dimensional $\varphi$ exists we may treat the path as having an effective finite memory.
                    </p>

                    <p>
                    <strong>Practical (modeling) formulations.</strong> The sufficiency declaration above admits common operational forms used in autoregressive modelling:
                    </p>

                    <ul>
                    <li><em>Order-$k$ Markov approximation</em> (discrete times): choose
                    $$\varphi(T_t) = \big(\gamma(t),\gamma(t-\Delta t),\dots,\gamma(t-(k-1)\Delta t)\big)\in\mathbb{R}^{nk},$$
                    and posit the generative model
                    $$\gamma(t+\Delta t) = f\big(\varphi(T_t)\big) + \varepsilon_{t+\Delta t},$$
                    where $\varepsilon$ is a noise term independent of $T_t$.</li>

                    <li><em>Continuous autoregressive / flow form:</em>
                        $$\dot\gamma(t) = F\big(\varphi(T_t),t\big) + \xi(t),$$
                        with $\xi$ a stochastic perturbation. Here $\varphi$ can encode short-term history or features of the whole past (see below).</li>

                    <li><em>Kernel / embedding summary:</em> choose an RKHS kernel $k:\mathbb{R}^n\times\mathbb{R}^n\to\mathbb{R}$ and use the mean embedding
                        $$\varphi(T_t) \;=\; \int_{a}^{t} k\big(\cdot,\gamma(s)\big)\,w(s)\,ds,$$
                        with weight $w(s)$ (e.g. exponential decay). Prediction is then performed in the RKHS via
                        $$\mathbb{E}[\gamma(t+\Delta)|T_t]\approx \mathcal{G}\big(\varphi(T_t)\big).$$</li>
                    </ul>

                    <p>
                    <strong>Proposition (sufficient condition).</strong>
                        If there exists a measurable finite-dimensional $\varphi$ and a measurable function $f$ such that
                        $$\gamma(t+\Delta)=f\big(\varphi(T_t),\zeta_{t+\Delta}\big)\qquad\text{a.s.},$$
                        with $\zeta_{t+\Delta}$ independent noise, then $\varphi(T_t)$ is sufficient in the sense above.
                    </p>

                    <p>
                    <strong>Remarks (interpretation & approximations).</strong>
                    <ul>
                    <li>The abstract sufficiency condition is measure-theoretic and nonconstructive; in practice we approximate $\varphi$ (and hence the sufficiency) by models that compress the past into a finite vector: sliding windows, exponential decays, learned recurrent states (RNN/LSTM/Transformer context vectors), or RKHS embeddings.</li>
                    <li>Verifying true minimal sufficiency from empirical data is generally infeasible; consequently we treat sufficiency as a modelling hypothesis to be evaluated by predictive performance and robustness to distributional shift.</li>
                    <li>When $\varphi(T_t)=\gamma(t)$ (the identity), we recover the ordinary Markov (order-1) hypothesis. When longer memory is needed, increase the capacity of $\varphi$ accordingly.</li>
                    </ul>
                    </p>

                    <p>
                    Together these statements make explicit the central claim used throughout this paper: <em>a path's traced vectors (or a sufficiently informative finite summary thereof) contain all the information practically required to form an optimal next-step prediction in the SVS.</em>
                    </p>
                    <h3> Probability Measures <h3>
                    <p>
                    If a feasible set of next steps exists, and we have information about which steps are most appealing, we can cast votes over where to go next, and these votes stem from our prior experience. I like to think of piles of paper ballots heaped over a flat plane. We are drawn to places where the weight of our past experiences weighs heavy with ballots, and we rarely tread in those sparsely balloted places.
                    </p>
                    <p>
                    These ballots are a probability measureabout which The idea that two points may be differently appealing as candidates for a path's progression invites us to talk about probability. A probability is a way of assigning confidence or preference to each outcome in a feasible set.
                    </p>
                    <p>
                    parameterise the path with time $\gamma(t)$<sup class="footnote-ref"><a href="#fn1" id="fnref1">1</a></sup>.
                    </p> 
                    <h2>Dynamic Plotting and Local Utility</h2>
                    <p>
                        If the endpoint is unset, the path must be plotted dynamically. The "map" is effectively being drawn as we walk it. The utility function $f_t(\mathbf{x}_t)$ becomes a local heuristic—a flashlight beam illuminating only the immediate surroundings in the semantic manifold.
                    </p>
                    <p>
                        This dynamic plotting raises a significant question: Does the lack of a fixed endpoint require a completely separate mathematical framework?
                    </p>
                    <p>
                         Standard path optimisation relies on global knowledge of the space. Dynamic traversal relies on local sensing and active decision-making. The topology of a space explored by an agent (Active Learning) is distinct from the topology of a space traversed by a projectile. It is likely that a separate article will be needed to formalise the specific geometry of these "exploratory" paths, where the metric tensor itself might be a function of the agent's uncertainty.
                    </p>

                    <h2>Relaxing the Welfare Function</h2>
                    <p>
                        Finally, we must address the "delicate thing" mentioned in the introduction: the welfare function $f_t(\mathbf{x}_t)$. In our formal definition, we assumed this was a well-behaved, smooth scalar field. In practice, the utility of a machine learning model is rarely so simple.
                    </p>
                    <p>
                        Real-world utility is a jagged, multi-objective landscape. We care about accuracy, but also inference latency, memory footprint, fairness, and interpretability. Often, these objectives are disjoint or directly contradictory (e.g., the trade-off between accuracy and sparsity).
                    </p>
                    <p>
                        Furthermore, the utility function is often non-stationary; what we value in the early stages of exploration (e.g., "does it learn at all?") differs from what we value at convergence (e.g., "is it robust to edge cases?"). A true generalist model cannot simply climb a static hill; it must infer the changing preferences of the experimentalist, effectively performing Inverse Reinforcement Learning to determine what "optimal" actually means in the current context.
                    </p>

                    <h2>The Ultimate Goal: Generalist Active Learning</h2>
                    <p>
                        Why bother with this topological pedantry? Why fuss over the definitions of paths and manifolds? 
                    </p>
                    <p>
                        We are not simply trying to train better classifiers for specific tasks. These essays are foundational steps toward a larger objective: building a <strong>Generalist, Multimodal, State-of-the-Art Active Learning Model</strong>.
                    </p>
                    <p class="font-medium text-stone-900">
                        Such a model acts as an autonomous traveller in experiment space. It cannot rely on human engineers to set the coordinates for every journey. It must understand the geometry of the semantic space itself to navigate efficiently—selecting its own data, tuning its own hyperparameters, and traversing from ignorance to generalisation with minimal compute. To build this model, we must first understand the ground it walks on.
                    </p>

            <section class="footnotes mt-10 pt-6 border-t border-gray-200">
                <h3 class="text-base font-semibold text-gray-700 mb-3">Notes</h3>
                <ol class="list-decimal pl-6 text-sm text-gray-500 space-y-2">
                    <li id="fn1">
                        It is interesting to imagine baking time into an SVS encoding. Then, the path parameter would be a real number that could index a path that retrogresses through time. 
                        In reality, though, experiments proceed unidirectionally in time, so encoding time into a semantic vector path may be arbitrary if time increases in a 1-to-1 relation with the path parameter.
                        If, however, (i) time progresses differently for two experiments, or (ii) the concept of time is entangled meaningfully with other bases, then its inclusion in the SVS encoding is necessary to comprehensibly describe an experiment.
                        
                        <a href="#fnref1" class="article-link ml-1">↩︎</a>
                    </li>
                </ol>
            </section>
            </div>
        </article>
    </main>

    <footer class="bg-white border-t border-gray-200 mt-12">
        <div class="container mx-auto px-6 py-8 text-center">
            <div class="flex justify-center space-x-8 mb-4">
                <a id="footer-email-link" href="#" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">Email</a>
                <a id="footer-linkedin-link" href="#" target="_blank" rel="noopener noreferrer" class="text-gray-500 hover:text-gray-900 transition duration-300 font-medium">LinkedIn</a>
            </div>
            <p class="text-gray-400 text-sm">&copy; 2025 Pascal Tohouri. All Rights Reserved.</p>
        </div>
    </footer>

    <script>
        window.addEventListener('DOMContentLoaded', () => {
            // --- KaTeX Initialization ---
            if (window.renderMathInElement && window.katex) {
                renderMathInElement(document.body, {
                    delimiters: [
                        {left: '$$', right: '$$', display: true},
                        {left: '$', right: '$', display: false},
                        {left: '\\(', right: '\\)', display: false},
                        {left: '\\[', right: '\\]', display: true}
                    ],
                    throwOnError: false
                });
            } else {
                console.warn("KaTeX or renderMathInElement not loaded. Math rendering may be incomplete.");
            }

            // --- Reading time calculation ---
            const articleBody = document.querySelector('.article-body');
            const readingTimeSpan = document.getElementById('reading-time');
            
            if (articleBody && readingTimeSpan) {
                const text = articleBody.innerText || articleBody.textContent || '';
                const words = text.trim().split(/\s+/).filter(Boolean).length;
                
                const defaultWpm = 200;
                const wpmAttr = readingTimeSpan.getAttribute('data-wpm');
                const WORDS_PER_MINUTE = wpmAttr ? parseInt(wpmAttr, 10) || defaultWpm : defaultWpm;
                
                const minutes = Math.max(1, Math.round(words / WORDS_PER_MINUTE));
                
                readingTimeSpan.textContent = `${minutes} Min Read`;
            }

            // --- Text-to-speech reader ---
        const ttsButton = document.getElementById('tts-toggle');

        if (ttsButton && 'speechSynthesis' in window && articleBody) {
            let utterance = null;
            let isSpeaking = false;

            ttsButton.addEventListener('click', () => {
                if (!isSpeaking) {
                    const text = articleBody.innerText || articleBody.textContent || '';
                    if (!text.trim()) return;

                    utterance = new SpeechSynthesisUtterance(text);
                    utterance.rate = 0.95; // slightly slower than default
                    utterance.onend = () => {
                        isSpeaking = false;
                        ttsButton.textContent = '▶ Listen';
                    };

                    isSpeaking = true;
                    ttsButton.textContent = '■ Stop';
                    window.speechSynthesis.speak(utterance);
                } else {
                    window.speechSynthesis.cancel();
                    isSpeaking = false;
                    ttsButton.textContent = '▶ Listen';
                }
            });
        } else if (ttsButton) {
            // Hide button if TTS not supported
            ttsButton.style.display = 'none';
        }

            // --- Obfuscated Contact Info ---
            const emailUser = 'pascal.tohouri1';
            const emailDomain = 'gmail.com';
            const linkedinUrl = 'https://www.linkedin.com/in/pascal-tohouri/';
            const fullEmail = `mailto:${emailUser}@${emailDomain}`;
            
            const footerEmailLink = document.getElementById('footer-email-link');
            const footerLinkedinLink = document.getElementById('footer-linkedin-link');
            if(footerEmailLink) footerEmailLink.href = fullEmail;
            if(footerLinkedinLink) footerLinkedinLink.href = linkedinUrl;
        });
    </script>
</body>
</html>
